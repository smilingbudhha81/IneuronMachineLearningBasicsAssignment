{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth.\n",
    "\n",
    "\n",
    "\n",
    "ANS = \n",
    "\n",
    "Using domain knowledge of data to transform features into higher or lower dimension and to create new features from existing features is feature engineering.\n",
    "\n",
    "Various aspects of feature engineering include:\n",
    "\n",
    "Understanding the problem statement.\n",
    "Train a baseline model and calculate its performance metric\n",
    "Reaching a conclusion about what features to create\n",
    "Create features\n",
    "Calculating how good new features contribute by training and testing and comparing the baseline performance metric\n",
    "Improve the features if need be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection?\n",
    "\n",
    "\n",
    "\n",
    "ANS = \n",
    "\n",
    "\n",
    " Feature Selection is the process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in. Having irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features.\n",
    "\n",
    "There are three types of feature selection:\n",
    "\n",
    "Wrapper methods (forward, backward, and stepwise selection)\n",
    "Filter methods (ANOVA, Pearson correlation, variance thresholding)\n",
    "Embedded methods (Lasso, Ridge, Decision Tree)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?\n",
    "\n",
    "\n",
    "\n",
    "ANS = \n",
    "\n",
    "\n",
    "Filter methods rely on finding the statistics for each feature to select the features with highest contributions to the machine learning model. These statistics are calculated for both categorical and numerical data. For numerical data, statistics like Correlation coefficients and for categorical data, statistics like the Chi-Square test are applied between two features to find probability of correlation(linear and non-linear).\n",
    "\n",
    "Advantage:--- Less computational cost Disadvantage:--- Less accurate, cannot handle multicollinearity\n",
    "\n",
    "Wrapper methods rely on an iterative approach where model is trained over subsets of features and tested using a performance metric, using which we can decide to keep features or discard them by comparing the recent performance score with the last score.\n",
    "\n",
    "Advantage:--- Most accurate Disadvantage:--- Computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.\n",
    "\n",
    "i. Describe the overall feature selection process.\n",
    "\n",
    "ii. Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?\n",
    "\n",
    "\n",
    "\n",
    "ANS = \n",
    "\n",
    "\n",
    " Feature selection is the process of reducing the number of input variables when developing a predictive model. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Describe the feature engineering process in the sense of a text categorization issue.\n",
    "\n",
    "\n",
    "\n",
    "ANS = \n",
    "\n",
    "Text categorization or text classification based on tags requires a feature engineering because machine cannot understand words.\n",
    "\n",
    "After the corpus has been converted to a numerical data using TF-IDF or Bag of Words, feature engineering technique is applied so that parts of speech tagging is more accurate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine.\n",
    "\n",
    "\n",
    "\n",
    "ANS = \n",
    "\n",
    "\n",
    "Cosine similarity is a metric used to measure how similar the documents are irrespective of their size. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together.\n",
    "\n",
    "Cosine similarity is the cosine of the angle between two n-dimensional vectors in an n-dimensional space. It is the dot product of the two vectors divided by the product of the two vectors' lengths (or magnitudes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator = (2*2)+(3*1)+(2*0)+(0*0)+(2*3)+(3*2)+(3*1)+(0*3)+(1*1)\n",
    "denominator = sqrt(4+9+4+4+9+9+1)*sqrt(4+1+9+4+1+9+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6753032524419089\n"
     ]
    }
   ],
   "source": [
    "similarity = numerator/denominator\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.\n",
    "\n",
    "i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap.\n",
    "\n",
    "ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0, 0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS = \n",
    "\n",
    "\n",
    "Thus the Hamming distance between two vectors is the number of bits we must change to change one into the other. Example Find the distance between the vectors 01101010 and 11011011. They differ in four places, so the Hamming distance d(01101010,11011011) = 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. State what is meant by  \"high-dimensional data set\"? Could you offer a few real-life examples? What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?\n",
    "\n",
    "\n",
    "\n",
    "ANS = \n",
    "\n",
    " High dimension is when variable numbers p is higher than the sample sizes n i.e. p>n, cases. High dimensional data is referred to a data of n samples with p features, where p is larger than n.\n",
    "\n",
    "For example, tomographic imaging data, ECG data, and MEG data. One example of high dimensional data is microarray gene expression data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Make a few quick notes on:\n",
    "\n",
    "PCA is an acronym for Personal Computer Analysis.\n",
    "2. Use of vectors\n",
    "\n",
    "3. Embedded technique\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS = \n",
    "\n",
    "The Principal component analysis (PCA) is a technique used for identification of a smaller number of uncorrelated variables known as principal components from a larger set of data. The technique is widely used to emphasize variation and capture strong patterns in a data set.\n",
    "\n",
    "Vectors can be used to represent physical quantities. Most commonly in physics, vectors are used to represent displacement, velocity, and acceleration. Vectors are a combination of magnitude and direction, and are drawn as arrows\n",
    "\n",
    "In the context of machine learning, an embedding is a low-dimensional, learned continuous vector representation of discrete variables into which you can translate high-dimensional vectors. Generally, embeddings make ML models more efficient and easier to work with, and can be used with other models as well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Make a comparison between:\n",
    "\n",
    "1. Sequential backward exclusion vs. sequential forward selection\n",
    "\n",
    "2. Function selection methods: filter vs. wrapper\n",
    "\n",
    "3. SMC vs. Jaccard coefficient\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS = \n",
    "\n",
    "i. Sequential backward exclusion vs. sequential forward selection\n",
    "sequential forward selection:--- Against a defined metric, first feature is chosen after a model is trained and produces the metric. Next feature is selected by searching for that feature which when clubbed with the first chosen feature gives best performance metric results via model training. More and more features are selected one by one while measuring the contribution of the clubbed features in each iteration against the metric using model trained on clubbed features till the required number of features are selected.\n",
    "\n",
    "Sequential backward exclusion:--- A model is trained on all features and the metric score is calculated. In each iteration, a feature is found which if removed from the collection of features, improves the metric score. This goes on and features are discarded one by one, improving performance metric score with each iteration until predefined number of features are left.\n",
    "\n",
    "ii. Function selection methods: filter vs. wrapper\n",
    "Filter methods rely on finding the statistics for each feature to select the features with highest contributions to the machine learning model. These statistics are calculated for both categorical and numerical data. For numerical data, statistics like Correlation coefficients and for categorical data, statistics like the Chi-Square test are applied between two features to find probability of correlation(linear and non-linear).\n",
    "\n",
    "Wrapper approach is an iterative approach in feature selection where all combinations of subset of features is used to train the model and a perfomance metric is used to test the model and check of there is an improvement of score over the previous iteration where a different subset of features were used. Examples of this technique are forward feature selection and backward feature selection.\n",
    "\n",
    "iii. SMC vs. Jaccard coefficient\n",
    "SMC or Simple Matching Index is defined as the number of matching attributes divided by total number of attributes. SMC can only be applied to observations with equal number of data points. SMC is most useful in binary attributes.\n",
    "\n",
    "Jaccard coefficient, also called Jaccard index is a measure of similarity of sets. It is defined as Modulus of A intersection B divided by modulus of A union B.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
